logistic 回归模型是目前广泛使用的学习算法之一，通常用来解决二分类问题，虽然名字中有“回归”，但它是一个分类算法。有些文献中译为“逻辑回归”，但中文“逻辑”与 logistic 和 logit 的含义相去甚远，因此本文中直接使用 logistic 表示。

以肿瘤分类为例，我们要预测肿瘤是否为恶性肿瘤，我们用 0 和 1 表示这两个取值，用 0 表示不是恶性肿瘤，用 1 表示是恶性肿瘤（当然也可以反过来，没有影响），那么数据集如图所示。

![42](/Users/sunshuai/Desktop/machine-learning/image/42.png)

假如用线性回归模型来拟合的话，看起来可能会是这样的。

![43](/Users/sunshuai/Desktop/machine-learning/image/43.png)

我们可以设置阈值为 0.5，如果输出小于等于 0.5，则预测 y 为 0；输出大于等于 0.5 则预测 y 为 1。

![44](/Users/sunshuai/Desktop/machine-learning/image/44.png)

这样红点左边的都被预测为 0，红点右边的都被预测为 1，我们得到了想要的结果。在这个例子中，使用线性回归似乎很合理，即使这是一个分类问题而不是一个回归问题。

但是我们修改一下数据集，将横轴延长，在右边增加一个点，如图。

![45](/Users/sunshuai/Desktop/machine-learning/image/45.png)

继续使用线性回归拟合的话，会得到图中蓝色这样一条直线。

![46](/Users/sunshuai/Desktop/machine-learning/image/46.png)

如果将阈值设为 0.5 的话，蓝点左边会被预测为 0，蓝点右边会被预测为 1。

![47](/Users/sunshuai/Desktop/machine-learning/image/47.png)

显然，此时的线性回归模型表现很差，因为没有很好的拟合数据集。而且使用线性回归还会出现输出值远小于 0 或者远大于 1 的情况。下面我们介绍 logistic 回归模型，它的输出值会介于 0 和 1 之间，不会大于 1，或者小于 0。



我们希望分类器的输出在 0 和 1 之间，因此我们将线性回归得到的输出再做一步计算，使用 sigmoid 函数。sigmoid 函数的公式为：

![48](/Users/sunshuai/Desktop/machine-learning/image/48.png)

图像为：

![49](/Users/sunshuai/Desktop/machine-learning/image/49.jpeg)

从图像可以看出，sigmoid 函数将负无穷到正无穷之间的数转化为 0 到 1 之间，负无穷处趋向 0 ，正无穷处趋向 1，原点处为 0.5。结合线性回归的假设函数

![50](/Users/sunshuai/Desktop/machine-learning/image/50.png)

得到 logistic 回归的假设函数

![51](/Users/sunshuai/Desktop/machine-learning/image/51.png)

上面说了它的输出都是在 0 到 1 之间，含义是“给出 x 时，y=1的概率”。用公式表示为

![52](/Users/sunshuai/Desktop/machine-learning/image/52.png)

然后使用数据集拟合 logistic 回归模型，得到参数 W。

在线性回归中，我们使用的代价函数为

![53](/Users/sunshuai/Desktop/machine-learning/image/53.png)

转化一下，

![54](/Users/sunshuai/Desktop/machine-learning/image/54.png)

则

![55](/Users/sunshuai/Desktop/machine-learning/image/55.png)

可以理解为，在模型预测的结果 h(x) 和 实际标签 y 相同时，即预测正确时，我们希望模型付出的代价，或者说用这么大的代价来惩罚模型。

最小均方误差的代价函数在线性回归中很好用，但是在 logistic 回归中，如果我们可以最小化这个函数，则模型可以工作。但实际上，在 logistic 回归中，最小均方误差代价函数是参数 W 的非凸函数。形如

![56](/Users/sunshuai/Desktop/machine-learning/image/56.png)

因此我们要换一个代价函数，通过统计学中极大似然估计，对于单个训练样本，公式为

![57](/Users/sunshuai/Desktop/machine-learning/image/57.png)

当 y=1 时，

![58](/Users/sunshuai/Desktop/machine-learning/image/58.png)

从图中可以直观地看出，当假设函数趋于 1 时，代价函数趋于 0，表示预测值与真实值相等，假设函数预测正确，我们不需要模型付出代价。而当假设函数趋于 0 时，与实际值 1 完全不同，代价函数趋于正无穷，表示我们需要模型付出非常大的代价。

当 y=0 时，

![59](/Users/sunshuai/Desktop/machine-learning/image/59.png)

含义同理。

将 y=1 和 y=0 的情况合并，可以简化为

![60](/Users/sunshuai/Desktop/machine-learning/image/60.png)

整个训练样本上的代价函数为

![61](/Users/sunshuai/Desktop/machine-learning/image/61.png)

这样就可以使用梯度下降来求解最小值。





TODO：1.极大似然估计推导2.梯度下降推导